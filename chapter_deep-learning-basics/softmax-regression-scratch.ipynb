{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax 回归的从零开始实现\n",
    "\n",
    "这一节我们来动手实现 Softmax 回归。首先导入本节实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gluonbook as gb\n",
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取和读取数据\n",
    "\n",
    "我们将使用 Fashion-MNIST 数据集，并设置批量大小为 256。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/forestriveral/.mxnet/datasets/fashion-mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-images-idx3-ubyte.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/forestriveral/.mxnet/datasets/fashion-mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/forestriveral/.mxnet/datasets/fashion-mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-images-idx3-ubyte.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/forestriveral/.mxnet/datasets/fashion-mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "跟线性回归中的例子一样，我们将使用向量表示每个样本。已知每个样本输入是高和宽均为 28 像素的图像。模型的输入向量的长度是 $28 \\times 28 = 784$：该向量的每个元素对应图像中每个像素。由于图像有 10 个类别，单层神经网络输出层的输出个数为 10。所以 Softmax 回归的权重和偏差参数分别为 $784 \\times 10$ 和 $1 \\times 10$ 的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))\n",
    "b = nd.zeros(num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同之前一样，我们要对模型参数附上梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "W.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现 Softmax 运算\n",
    "\n",
    "在介绍如何定义 Softmax 回归之前，我们先描述一下对如何对多维 NDArray 按维度操作。在下面例子中，给定一个 NDArray 矩阵`X`。我们可以只对其中同一列（`axis=0`）或同一行（`axis=1`）的元素求和，并在结果中保留行和列这两个维度（`keepdims=True`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n [[ 5.  7.  9.]]\n <NDArray 1x3 @cpu(0)>, \n [[  6.]\n  [ 15.]]\n <NDArray 2x1 @cpu(0)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[1, 2, 3], [4, 5, 6]])\n",
    "X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们就可以定义前面小节里介绍的 softmax 运算了。在下面的函数中，矩阵`X`的行数是样本数，列数是输出个数。为了表达样本预测各个输出的概率，softmax 运算会先通过`exp`函数对每个元素做指数运算，再对`exp`矩阵同行元素求和，最后令矩阵每行各元素与该行元素之和相除。这样一来，最终得到的矩阵每行元素和为 1 且非负。因此，该矩阵每行都是合法的概率分布。Softmax 运算的输出矩阵中的任意一行元素代表了一个样本在各个输出类别上的预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，对于随机输入，我们将每个元素变成了非负数，且每一行和为 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n [[ 0.21324193  0.33961776  0.1239742   0.27106097  0.05210521]\n  [ 0.11462264  0.3461234   0.19401033  0.29583326  0.04941036]]\n <NDArray 2x5 @cpu(0)>, \n [ 1.00000012  1.        ]\n <NDArray 2 @cpu(0)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.random.normal(shape=(2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "有了 softmax 运算，我们可以定义上节描述的 softmax 回归模型了。这里通过`reshape`函数将每张原始图像改成长度为`num_inputs`的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "\n",
    "上一节中，我们介绍了 softmax 回归使用的交叉熵损失函数。为了得到标签的预测概率，我们可以使用`pick`函数。在下面例子中，变量`y_hat`是 2 个样本在 3 个类别的预测概率，变量`y`是这 2 个样本的标签类别。通过使用`pick`函数，我们得到了 2 个样本的标签的预测概率。与[“Softmax 回归”](softmax-regression.md)一节数学表述中标签类别离散值从 1 开始逐一递增不同，在代码中，标签类别的离散值是从 0 开始逐一递增的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n[ 0.1  0.5]\n<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = nd.array([0, 2])\n",
    "nd.pick(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下实现了[“Softmax 回归”](softmax-regression.md)一节中介绍的交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - nd.pick(y_hat, y).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算分类准确率\n",
    "\n",
    "给定一个类别的预测概率分布`y_hat`，我们把预测概率最大的类别作为输出类别。如果它与真实类别`y`一致，说明这次预测是正确的。分类准确率即正确预测数量与总预测数量之比。\n",
    "\n",
    "下面定义准确率`accuracy`函数。其中`y_hat.argmax(axis=1)`返回矩阵`y_hat`每行中最大元素的索引，且返回结果与变量`y`形状相同。我们在[“数据操作”](../chapter_prerequisite/ndarray.md)一节介绍过，相等条件判断式`(y_hat.argmax(axis=1) == y)`是一个值为 0（相等为假）或 1（相等为真）的 NDArray。由于标签类型为整数，我们先将变量`y`变换为浮点数再进行相等条件判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们继续使用在演示`pick`函数时定义的变量`y_hat`和`y`，并将它们分别作为预测概率分布和标签。可以看到，第一个样本预测类别为 2（该行最大元素 0.6 在本行的索引为 2），与真实标签 0 不一致；第二个样本预测类别为 2（该行最大元素 0.5 在本行的索引为 2），与真实标签 2 一致。因此，这两个样本上的分类准确率为 0.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们可以评价模型`net`在数据集`data_iter`上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "# 本函数已保存在 gluonbook 包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增\n",
    "# 广”一节中描述。\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc = 0\n",
    "    for X, y in data_iter:\n",
    "        acc += accuracy(net(X), y)\n",
    "    return acc / len(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们随机初始化了模型`net`，所以这个随机模型的准确率应该接近于类别个数 10 的倒数 0.1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0947265625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_iter, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "训练 softmax 回归的实现跟前面介绍的线性回归中的实现非常相似。我们同样使用小批量随机梯度下降来优化模型的损失函数。在训练模型时，迭代周期数`num_epochs`和学习率`lr`都是可以调的超参数。改变它们的值可能会得到分类更准确的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7878, train acc 0.745, test acc 0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 0.5742, train acc 0.811, test acc 0.823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.5290, train acc 0.824, test acc 0.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.5054, train acc 0.830, test acc 0.837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.4895, train acc 0.834, test acc 0.840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.4788, train acc 0.837, test acc 0.843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 0.4703, train acc 0.841, test acc 0.846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 0.4625, train acc 0.843, test acc 0.846\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 8, 0.1\n",
    "\n",
    "# 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum = 0\n",
    "        train_acc_sum = 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                gb.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)  # 下一节将用到。\n",
    "            train_l_sum += l.mean().asscalar()\n",
    "            train_acc_sum += accuracy(y_hat, y)\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / len(train_iter),\n",
    "                 train_acc_sum / len(train_iter), test_acc))\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs,\n",
    "          batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "训练完成后，现在我们可以演示如何对图像进行分类。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\nTraceback (most recent call last):\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 175, in fetcher_loop\n    idx, batch = data_queue.get()\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n    return _ForkingPickler.loads(res)\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 56, in rebuild_ndarray\n    fd = fd.detach()\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n    answer_challenge(c, authkey)\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n    response = connection.recv_bytes(256)        # reject large message\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/home/forestriveral/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nConnectionResetError: [Errno 104] Connection reset by peer\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 9 Axes>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in test_iter:\n",
    "    break\n",
    "\n",
    "true_labels = gb.get_fashion_mnist_labels(y.asnumpy())\n",
    "pred_labels = gb.get_fashion_mnist_labels(net(X).argmax(axis=1).asnumpy())\n",
    "titles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n",
    "\n",
    "gb.show_fashion_mnist(X[0:9], titles[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    " \n",
    "* 我们可以使用 softmax 回归做多类别分类。与训练线性回归相比，你会发现训练 softmax 回归的步骤和它非常相似：获取并读取数据、定义模型和损失函数并使用优化算法训练模型。事实上，绝大多数深度学习模型的训练都有着类似的步骤。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 本节中，我们直接按照 softmax 运算的数学定义来实现 softmax 函数。这可能会造成什么问题？（提示：试一试计算 $\\exp(50)$ 的大小。）\n",
    "* 本节中的`cross_entropy`函数是按照交叉熵损失函数的数学定义实现的。这样的实现方式可能有什么问题？（提示：思考一下对数函数的定义域。）\n",
    "* 你能想到哪些办法来解决上面的两个问题？\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/741)\n",
    "\n",
    "![](../img/qr_softmax-regression-scratch.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
